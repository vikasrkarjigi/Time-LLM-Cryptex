import subprocess
import pandas as pd
import numpy as np
import mlflow
import os
import sqlite3
import json
from pathlib import Path

def perform_inference(model_id, llm_model, inf_path, save_path, experiment_name):
    """
    Pipeline for the TimeLLM model.

    args:
        model_id: model id
        llm_model: llm model
        inf_path: path to the inference data
        save_path: path to save the inference data
    """
    print("\n==============================================")
    print(f"\nRunning inference for {model_id} with {llm_model} on {inf_path}")
    print("\n==============================================\n")


    cmd = f"python run_inference.py --model_id {model_id} --llm_model {llm_model} --data_path {inf_path} --save_path {save_path} --experiment_name {experiment_name}"
    subprocess.run(cmd, shell=True)


def perform_backtest(inf_output_path, optimize=False):
    """
    Perform backtest on the inference data.
    This function saves the summary table to the temp folder.

    args:
        inf_output_path: path to the inferenced data in candlestick format
    """
    print("\n==============================================")
    print(f"\nPerforming backtest for {inf_output_path}")
    print("\n==============================================\n")


    if optimize:
        cmd = f"python backtesting/backtest.py --data {inf_output_path} --walk_forward 12 --optimize BollingerAI --pipeline"
    else:
        cmd = f"python backtesting/backtest.py --data {inf_output_path} --pipeline"

    subprocess.run(cmd, shell=True)

    summary_table = pd.read_csv("summary_table.csv")
    
    return summary_table


def inf_analysis(inf_path):
    """
    Perform analysis on the inference data.

    args:
        client: mlflow client
        new_data_path: path to the new data
    """

    data = pd.read_csv(inf_path).dropna()
    pred_len = data.columns.str.contains('predicted').sum()

    mda_vals = {}

    
    for pred in range(1, pred_len+1):
        pred_col = f'close_predicted_{pred}'
        
        if pred_col not in data.columns:
            print(f"Column {pred_col} not found in data.")
            continue
        
        # We can only evaluate up to len(data) - pred positions
        # because we need pred steps of future actual values
        valid_positions = len(data) - pred
        
        if valid_positions <= 0:
            print(f"Not enough data to evaluate prediction horizon {pred}")
            continue
        
        # Current actual values (where predictions were made)
        current_actual = data['close'].iloc[:valid_positions].values
        
        # Predicted values for pred steps ahead
        predicted_values = data[pred_col].iloc[:valid_positions].values
        
        # Actual future values (pred steps ahead)
        future_actual = data['close'].iloc[pred:pred+valid_positions].values
        
        predicted_direction = predicted_values - current_actual
        
        actual_direction = future_actual - current_actual
        
        correct_direction = (predicted_direction * actual_direction) > 0
        
        # Calculate directional accuracy
        mda = correct_direction.mean()
        mda_vals[f'inf_pred_{pred}_mda'] = mda

    return mda_vals


def create_metrics_json(mlflow_run_id, llm_model, experiment_name, summary_table, mda_vals, trial_dict):
    """
    Create the metrics dataframe.

    args:
        summary_table: summary table
        mda_vals: mda values
    """
    metrics_dict = {}

    # Creates two metrics. The next candle prediction and the last candle prediction.

    metrics_dict["mlflow_run_id"] = mlflow_run_id
    metrics_dict["llm_model"] = llm_model
    metrics_dict["experiment_name"] = experiment_name
    metrics_dict["summary_table"] = summary_table.to_dict()
    metrics_dict["trial_parameters"] = trial_dict
    metrics_dict["inf_analysis"] = mda_vals

    metrics_json = json.dumps(metrics_dict)

    return metrics_json

def metrics_to_db(metrics_db_path, model_id, metrics_json):
    """
    Save metrics to the database as a JSON string.
    This is done to avoid the need to create a new table for each model.
    The metrics are stored in a JSON string so that they can be easily queried and analyzed.

    Args:
        metrics_db_path: path to the metrics database
        model_id: unique model identifier (primary key)
        metrics_json: dict of metrics (will be stored as JSON)
    """

    # Connects to the database
    print(f"Connecting to the metrics database at {metrics_db_path}\n\n")
    db = sqlite3.connect(metrics_db_path)
    cursor = db.cursor()

    try:
        # Creates the table if it doesn't exist
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS metrics (
                model_id TEXT PRIMARY KEY,
                metrics JSON
            )
        """)
    except Exception as e:
        print(f"Failed to create metrics table in {metrics_db_path}: \n{e}")
        raise ValueError(f"Failed to create metrics table in {metrics_db_path}: \n{e}")

    try:
        # Creates the table if it doesn't exist
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS metrics (
                model_id TEXT PRIMARY KEY,
                metrics JSON
            )
        """)
    except Exception as e:
        print(f"Failed to insert metrics into {metrics_db_path}: \n{e}")
        raise ValueError(f"Failed to insert metrics into {metrics_db_path}")

    # Inserts the metrics into the database
    cursor.execute("""
        INSERT OR REPLACE INTO metrics (model_id, metrics)
        VALUES (?, ?)
    """, (model_id, json.dumps(metrics_json)))

    print(f"Metrics inserted into {metrics_db_path} for model {model_id}")
    db.commit()
    db.close()

def aggregate_data(data, aggregate):
    """
    Aggregate OHLCV data from the original granularity to the specified granularity.
    Saves to save_path

    args:
        data_path: path to the data
        aggregate: aggregate period (e.g., '5' for 5 minutes from 1 minute, '60' for 1 hour from 1 minute, '1440' for 1 day from 1 minute)
    returns:
        Path to the aggregated data file (save_path)
    """
    if 'timestamp' not in data:
        raise ValueError("Missing 'timestamp' column")

    unix = data['timestamp'].dtype in ('int64', 'float64')
    data['timestamp'] = pd.to_datetime(data['timestamp'], unit='s', utc=True) if unix else pd.to_datetime(data['timestamp'])
    data.set_index('timestamp', inplace=True)

    # Convert numeric aggregate to pandas frequency string (e.g., 5 -> '5T' for 5 minutes)
    freq = f'{aggregate}T' if isinstance(aggregate, (int, float)) else aggregate

    agg = {k: v for k, v in {
        'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'
    }.items() if k in data}
    agg.update({c: 'mean' for c in data if c not in agg})

    out = data.resample(freq).agg(agg).dropna().reset_index()
    if unix:
        out['timestamp'] = out['timestamp'].astype('int64') // 10**9

    return out

def convert_to_returns(data_path, keep_high_low=False, keep_volume=True, log_returns=False):
    """
    Convert data to returns.

    args:
        data_path: path to the data
        log_returns: bool, if True, the data is converted to log returns
        keep_high_low: bool, if True, the high and low prices are kept
        keep_volume: bool, if True, the volume column is kept
    returns:
        Path to the returns data file (data_path)
    """
    # Checks if the data path is a file or a directory and saves the output path accordingly

    try:
        data = pd.read_csv(data_path)
    except:
        raise ValueError(f"Data path {data_path} is not a valid file.")
    try:
        data = pd.DataFrame({"close": data["close"], "volume": data["volume"], "timestamp": data["timestamp"]})
    except:
        print("-------------ERROR-------------------")
        print(data.head())
        print("-------------ERROR-------------------")
        raise ValueError(f"Data path {data_path} is not a valid file.")
    if log_returns:
        data["returns"] = np.log(data["close"] / data["close"].shift(1))
    else:
        data["returns"] = data["close"] / data["close"].shift(1) - 1
    
    data = data.dropna().reset_index(drop=True)

    final_data = pd.DataFrame()
    final_data['returns'] = data['returns']

    if keep_high_low:
        final_data["high"] = data["high"]
        final_data["low"] = data["low"]

    if keep_volume:
        final_data["volume"] = data["volume"]

    final_data["timestamp"] = data["timestamp"]

    final_data.to_csv(data_path, index=False)

    return data_path

def convert_back_to_candlesticks(original_data_path, inferenced_data_path, num_predictions):
    """
    Convert returns data back to candlesticks. This is used to backtest the model.
    Writes the inferenced data to the inferenced data path.

    args:
        original_data_path: path to the original candlestick data 
        inferenced_data_path: path to save the inferenced data
    """
    if not os.path.exists(Path(inferenced_data_path)):
        raise ValueError(f"Inference data path {inferenced_data_path} does not exist.")

    if not os.path.exists(original_data_path):
        raise ValueError(f"Original data path {original_data_path} does not exist.")

    candlesticks = pd.read_csv(original_data_path)
    predicted_returns = pd.read_csv(inferenced_data_path)

    # Make a copy of the candlesticks data
    result = candlesticks.copy()

    # Get the last known close price before predictions start
    try:
        last_close = result.loc[result.index[predicted_returns['returns_predicted_1'].first_valid_index()-1], 'close']
    except:
        print(result.head())
        print("--------------------------------")
        print(predicted_returns.head())
        raise ValueError(f"No valid close price found in the original data.")

    for i in range(1, num_predictions+1):  
        col = f'returns_predicted_{i}'
        if col in predicted_returns.columns:
            # Calculate cumulative returns 
            pred_close = last_close * (1 + predicted_returns[col])
            # Rename column
            result[f'close_predicted_{i}'] = pred_close

    # Convert unix timestamp to UTC datetime
    result["timestamp"] = pd.to_datetime(result["timestamp"], unit='s', utc=True)

    result.to_csv(inferenced_data_path, index=False)
    print(f"Predicted returns saved to {inferenced_data_path}")

    return inferenced_data_path
    
