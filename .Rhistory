rs <- roc.multi[['rocs']]
plot.roc(rs[[1]])
sapply(2:length(rs),function(i) lines.roc(rs[[i]],col=i))
roc.multi = multiclass.roc(actual_labels, yhat_df)
rs <- roc.multi[['rocs']]
rs
plot.roc(rs[[1]])
sapply(2:length(rs),function(i) lines.roc(rs[[i]],col=i))
plot.roc(rs[[1]])
roc.multi = multiclass.roc(actual_labels, yhat_df)
rs <- roc.multi[['rocs']]
plot.roc(rs[[1]])
rs[[1]]
names(rc[[1]])
names(rs[[1]])
typeof(rs[[1]])
length(rs[[1]])
rs[[1]][2]
rs[[1]][1]
rs[[1]]
roc.multi = multiclass.roc(actual_labels, yhat_df)
rs <- roc.multi[['rocs']]
rs_flat <- unlist(rs, recursive = FALSE)
# Plot the first ROC
plot.roc(rs_flat[[1]], col = 1)
sapply(2:length(rs),function(i) lines.roc(rs[[i]],col=i))
plot.roc(rs_flat[[1]], col = 1)
roc.multi = multiclass.roc(actual_labels, yhat_df)
rs <- roc.multi[['rocs']]
rs_flat <- unlist(rs, recursive = FALSE)
plot.roc(rs_flat[[1]], col = 1)
sapply(2:length(rs_flat), function(i) {
lines.roc(rs_flat[[i]], col = i)
})
plot_roc_OvR(probs = yhat_mat, actual = actual_labels)
roc.multi = multiclass.roc(actual_labels, yhat_df)
rs <- roc.multi[['rocs']]
rs_flat <- unlist(rs, recursive = FALSE)
plot.roc(rs_flat[[1]], col = 1)
sapply(2:length(rs_flat), function(i) {
lines.roc(rs_flat[[i]], col = i)
})
legend("bottomright", legend = paste0("ROC ", seq_along(rs_flat)), col = 1:length(rs_flat), lwd = 2)
source("~/Documents/iit/SP25/DPA/Big Project/Project/R_project/utils.R", echo=TRUE)
plot_roc_OvR(probs = yhat_df, actual = actual_labels)
source("~/Documents/iit/SP25/DPA/Big Project/Project/R_project/utils.R", echo=TRUE)
plot_roc_OvR(probs = yhat_df, actual = actual_labels)
yhat_mat = matrix(predict(model.rf, test), ncol = 3, nrow = length(yhat_probs), byrow = TRUE)
model.rf = ranger(CrashSeverity ~ ., train,
mtry = 48,
num.trees = 250,
num.threads = 16)
model.rf = ranger(CrashSeverity ~ ., train,
mtry = 4,
num.trees = 250,
num.threads = 16)
yhat_mat = matrix(predict(model.rf, test), ncol = 3, nrow = length(yhat_probs), byrow = TRUE)
yhat_mat = matrix(predict(model.rf, test), ncol = 3, nrow = length(test[,1]), byrow = TRUE)
split = get_train_test(data,"CrashSeverity", train_pct = .8, data_slice_pct = .1,
return_indices = FALSE, seed = 1)
train = split$train
test = split$test
model.rf = ranger(CrashSeverity ~ ., train,
mtry = 4,
num.trees = 250,
num.threads = 16)
yhat_mat = matrix(predict(model.rf, test), ncol = 3, nrow = length(test[,1]), byrow = TRUE)
yhats = predict(model.rf, test, response = 'class')
summary(model.rf)
table(predictions = yhats$predictions, actual = test[,1])
get_f1_vals(yhats$predictions, test[,1])
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(caret)
library(nnet)
library(DBI)
library(RSQLite)
library(dplyr)
library(rsample)
source("utils.R")
library(e1071)
library(xgboost)
library(synthpop)
library(ranger)
library(MLmetrics)
library(doParallel)
con <- dbConnect(SQLite(), "crashes.db")
# Get the column names
query <- "
SELECT *
FROM crashes;"
# ORDER BY RANDOM()
# LIMIT 500000;"
orig_data = dbGetQuery(con, query)
# Leaves just CrashSeverity target Column
orig_data = orig_data[,!(colnames(orig_data) %in% c("TotalInjured","CrashInjurySeverity"))]
# Close connection
dbDisconnect(con)
orig_data = orig_data[,!colnames(orig_data) %in% c("HourOfCrash","CrashYr")]
for (col in names(orig_data)) {
orig_data[[col]] <- factor(orig_data[[col]])
}
data = orig_data
table(data$CrashSeverity)
split = get_train_test(orig_data, "CrashSeverity",data_slice_pct = .01)
train = split$train
test = split$test
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold CV
n_cores <- parallel::detectCores()
split = get_train_test(orig_data, "CrashSeverity",data_slice_pct = .005)
train = split$train
test = split$test
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold CV
n_cores <- parallel::detectCores()
cl <- makePSOCKcluster(12)  # Leave 1 core free to keep system responsive
registerDoParallel(cl)
grid <- expand.grid(
cost = c(0.1, 1, 10)
)
svm.cv.linear <- train(
CrashSeverity ~ .,
data = train,
trControl = ctrl,
method = "svmLinear2",
tuneGrid = grid
)
stopCluster(cl)
registerDoSEQ()
cl <- makePSOCKcluster(12)  # Leave 1 core free to keep system responsive
registerDoParallel(cl)
svm.radial <- train(
CrashSeverity ~ .,
data = train,
method = "svmRadial",
tuneGrid = data.frame(C = 1, sigma = 0.01)
)
yhats = predict(svm.radial, newdata = test)
table(predicted = yhats, actual = test[,1])
get_f1_vals(test[,1],yhats)
stopCluster(cl)
registerDoSEQ()
split = get_train_test(orig_data, "CrashSeverity",data_slice_pct = .02, train_pct = .25)
train = split$train
test = split$test
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold CV
n_cores <- parallel::detectCores()
split = get_train_test(orig_data, "CrashSeverity",data_slice_pct = .01, train_pct = .5)
train = split$train
test = split$test
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold CV
n_cores <- parallel::detectCores()
yhats = predict(svm.radial, newdata = test)
table(predicted = yhats, actual = test[,1])
get_f1_vals(test[,1],yhats)
cl <- makePSOCKcluster(12)
registerDoParallel(cl)
svm.poly <- train(
CrashSeverity ~ .,
data = train,
method = "svmPoly",
tuneGrid = data.frame(degree = 2, scale = 0.1, C = 1.)
)
yhats = predict(svm.poly, newdata = test)
table(predicted = yhats, actual = test[,1])
get_f1_vals(test[,1],yhats)
stopCluster(cl)
registerDoSEQ()
colnames(data)
library(ts)
?arima.sim
arima.sim(list(order = c(1,0,0), ar = -.5))
arima.sim(list(order = c(1,0,0), ar = -.5), 50)
plot(arima.sim(list(order = c(1,0,0), ar = -.5), 50))
plot(arima.sim(list(order = c(2,0,0), ar = c(0,-.5)), 50))
library(forecast)
?forecast
forecast(data)
plot(data)
data = arima.sim(list(order = c(2,0,0), ar = c(0,-.5)), 50)
plot(data)
library(forecast)
forecast(data)
plot(data)
plot(forecast(data))
data = arima.sim(list(order = c(1,1,1), 50)
)
data = arima.sim(list(order = c(1,1,1), n = 50)
)
data = arima.sim(list(order = c(1,1,1), n = 50)))
?arima.sim
data = arima.sim(list(order = c(1,1,1)), n = 50)
plot(data)
library(forecast)
plot(acf(data))
data = arima.sim(list(order = c(1,1,1), ar = .5, ma = .5), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(1,1,1), ar = .5, ma = 0), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(2,1,2), ar = (.5,.6), ma = c(-.1,.2)), n = 50)
plot(data)
data = arima.sim(list(order = c(2,1,2), ar = (.5,.6), ma = c(-.1,.2)), n = 50)
data = arima.sim(list(order = c(2,1,2), ar = c(.5,.6), ma = c(-.1,.2)), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(2,1,2), ar = c(.5,-.6), ma = c(-.1,.2)), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(2,0,2), ar = c(.5,-.6), ma = c(-.1,.2)), n = 50)
data = arima.sim(list(order = c(2,0,2), ar = c(.5,-.6), ma = c(-.1,.2)), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(2,1,2), ar = c(.5,-.6), ma = c(-.1,.2)), n = 50)
plot(data)
plot(acf(data))
plot(data)
data = arima.sim(list(order = c(2,0,2), ar = c(.5,-.6), ma = c(-.1,.2)), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(2,1,2), ar = c(.5,-.6), ma = c(-.1,.2)), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(2,2,2), ar = c(.5,-.6), ma = c(-.1,.2)), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(2,1,2), ar = c(.5,-.6), ma = c(-.1,.2)), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(2,1,2), ar = c(.5,-.6), ma = c(-.1,.2)), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(2,1,2), ar = c(.5,-.6), ma = c(.1,-.2)), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(2,1,2), ar = c(.5,-.6), ma = c(.1,-.8)), n = 50)
plot(data)
plot(acf(data))
data = arima.sim(list(order = c(1,1,1), ar = c(.5), ma = c(.1)), n = 50)
plot(acf(data))
data = arima.sim(list(order = c(1,1,1), ar = c(.5), ma = c(.8)), n = 50)
plot(acf(data))
data = arima.sim(list(order = c(1,1,1), ar = c(.5), ma = c(.9)), n = 50)
print("Hello")
for (layer in hidden_layers) {
print("Hello")
}
for (layer in hidden_layers) {
print("Hello")
}
for (i in 1:length(hidden_layers)) {
hidden_layers[i]
print("Hello")
}
hidden_layers = c(20,15) # add the number nodes in each hidden layer
for (i in 1:length(hidden_layers)) {
hidden_layers[i]
print("Hello")
}
print(hidden_layers[i])
for (i in 1:length(hidden_layers)) {
print(hidden_layers[i])
print("Hello")
}
for (i in hidden_layers) {
print("Hello")
}
for (i in hidden_layers) {
tot += last + 1 * i
for (i in hidden_layers) {
tot = tot + (last + 1) * i
last = i
print("Hello")
}
tot = 0
for (i in hidden_layers) {
tot = tot + (last + 1) * i
last = i
print("Hello")
}
last = num_inputs
tot = 0
for (i in hidden_layers) {
tot = tot + (last + 1) * i
last = i
print("Hello")
}
last = num_inputs
tot = 0
for (i in hidden_layers) {
tot = tot + (last + 1) * i
last = i
print("Hello")
}
num_inputs = 10
hidden_layers = c(20,15) # add the number nodes in each hidden layer
outputs = 10
last = num_inputs
tot = 0
for (i in hidden_layers) {
tot = tot + (last + 1) * i
last = i
print("Hello")
}
tot
print(tot)
layers = c(10,20,15,10) # input, hidden layer 1,2,..., output layer
last = layers[1]
for (i in 2:length(layers)) {
layer = layers[i]
tot = tot + (last + 1) * layer
last = i
print("Hello")
}
layers = c(10,20,15,10) # input, hidden layer 1,2,..., output layer
tot = 0
last = layers[1]
for (i in 2:length(layers)) {
layer = layers[i]
tot = tot + (last + 1) * layer
last = i
print("Hello")
}
print(tot)
layers = c(10,20,15,10) # input, hidden layer 1,2,..., output layer
tot = 0
last = layers[1]
for (i in 2:length(layers)) {
layer = layers[i]
tot = tot + (last + 1) * layer
last = layer
}
print(tot)
layers = c(100,64,32,10) # input, hidden layer 1,2,..., output layer
tot = 0
last = layers[1]
for (i in 2:length(layers)) {
layer = layers[i]
tot = tot + (last + 1) * layer
last = layer
}
print(tot)
layers = c(3,4,2) # input, hidden layer 1,2,..., output layer
tot = 0
last = layers[1]
for (i in 2:length(layers)) {
layer = layers[i]
tot = tot + (last + 1) * layer
last = layer
}
print(tot)
layers = c(3,4,2) # input, hidden layer 1, hidden layer 2,..., output layer
tot = 0
last = layers[1]
for (i in 2:length(layers)) {
layer = layers[i]
tot = tot + (last + 1) * layer
last = layer
}
print(tot)
mean(x)
# 1.19
x = c(21, 14, 28, 28, 16, 28)
y = c(38.97, 3.885, 3.778, 3.914, 1.86, 2.948)
mean(x)
setwd("~/Documents/iit/FA25/research/Time-LLM-Cryptex")
library(tidyverse)
df <- as.tibble(read.csv("dataset/cryptex/params_analysis/llama3.1_ret-h-4m_params.csv", header = TRUE))
df <- as_tibble(read.csv("dataset/cryptex/params_analysis/llama3.1_ret-h-4m_params.csv", header = TRUE))
df <- df[df["State"] == "COMPLETE"]
df %>%
filter(Status == "COMPLETE")
df %>%
filter("Status" == "COMPLETE")
df
df %>%
filter("Status" == "COMPLETE") %>%
filter("Param.pred_len" != 2)
df %>%
filter("Status" == "COMPLETE") %>%
filter("Param.pred_len" != 2) %>%
select(-c("State", "Param.batch_size", "Number", "UserAttribute.dataset", "UserAttribute.granularity", "UserAttribute.target", "Param.pred_len"))
model <- lm(Value ~ ., data = df)
df %>%
filter("Status" == "COMPLETE") %>%
filter("Param.pred_len" != 2) %>%
select(-c("State", "Param.batch_size", "Number", "UserAttribute.dataset", "UserAttribute.granularity", "UserAttribute.target", "Param.pred_len"))
df <- as_tibble(read.csv("dataset/cryptex/params_analysis/llama3.1_ret-h-4m_params.csv", header = TRUE))
df %>%
filter("Status" == "COMPLETE") %>%
filter("Param.pred_len" != 2) %>%
select(-c("State", "Param.batch_size", "Number", "UserAttribute.dataset", "UserAttribute.granularity", "UserAttribute.target", "Param.pred_len"))
filtered_df = df %>%
filter("Status" == "COMPLETE") %>%
filter("Param.pred_len" != 2) %>%
select(-c("State", "Param.batch_size", "Number", "UserAttribute.dataset", "UserAttribute.granularity", "UserAttribute.target", "Param.pred_len"))
model <- lm(Value ~ ., data = filtered_df)
df
filtered_df
df <- as_tibble(read.csv("dataset/cryptex/params_analysis/llama3.1_ret-h-4m_params.csv", header = TRUE))
filtered_df = df %>%
filter("Status" == "COMPLETE") %>%
filter("Param.pred_len" != 2) %>%
select(-c(State, Param.batch_size, Number, UserAttribute.dataset, UserAttribute.granularity, UserAttribute.target, Param.pred_len))
filtered_df
df <- as_tibble(read.csv("dataset/cryptex/params_analysis/llama3.1_ret-h-4m_params.csv", header = TRUE))
df
filtered_df = df %>%
filter("Status" == "COMPLETE") %>%
filter("Param.pred_len" != 2) %>%
select(-c(State, Param.batch_size, Number, UserAttribute.dataset, UserAttribute.granularity, UserAttribute.target, Param.pred_len))
filtered_df = df %>%
filter("Status" == "COMPLETE") %>%
filter("Param.pred_len" != 2)
filtered_df = df %>%
filter("Status" == "COMPLETE") %>%
filter("Param.pred_len" != 2) %>%
select(-c(State, Param.batch_size, Number, UserAttribute.dataset, UserAttribute.granularity, UserAttribute.target, Param.pred_len))
filtered_df
`Param.pred_len`))
df <- as_tibble(read.csv("dataset/cryptex/params_analysis/llama3.1_ret-h-4m_params.csv", header = TRUE))
filtered_df = df %>%
filter("Status" == "COMPLETE") %>%
filter("Param.pred_len" != 2) %>%
select(-c(State, `Param.batch_size`, Number,
`UserAttribute.dataset`,
`UserAttribute.granularity`,
`UserAttribute.target`,
`Param.pred_len`))
filtered_df
filtered_df = df %>%
filter("Status" == "COMPLETE") %>%
filter("Param.pred_len" != 2)
filtered_df
filtered_df = df %>%
filter(Status == "COMPLETE")
rlang::last_trace()
names(filtered_df)
filtered_df = df %>%
filter("State" == "COMPLETE") %>%
filter("Param.pred_len" != 2) %>%
select(-c(State, `Param.batch_size`, Number,
`UserAttribute.dataset`,
`UserAttribute.granularity`,
`UserAttribute.target`,
`Param.pred_len`))
filtered_df
names(filtered_df)
names(df)
filtered_df = df %>%
filter("State" == "COMPLETE") %>%
filter("Param.pred_len" != 2)
filtered_df
filtered_df = df %>%
filter("State" == "COMPLETE")
filtered_df
df
filtered_df = df %>%
filter(State == "COMPLETE") %>%
filter("Param.pred_len" != 2) %>%
select(-c(State, `Param.batch_size`, Number,
`UserAttribute.dataset`,
`UserAttribute.granularity`,
`UserAttribute.target`,
`Param.pred_len`))
filtered_df
model <- lm(Value ~ ., data = filtered_df)
summary(model)
x_bar <- mean(df$Value)
n <- length(df$Value)
p_value <- pt(t_stat, n - 1, lower.tail = FALSE)
t_stat <- (x_bar - .5) / (sd(df$Value) / sqrt(length(df$Value)))
p_value <- pt(t_stat, n - 1, lower.tail = FALSE)
# Print t-statistic
# This indicates whether the parameter is significantly different from 0.5
print(t_stat)
df$Value
unfiltered_df <- as_tibble(read.csv("dataset/cryptex/params_analysis/llama3.1_ret-h-4m_params.csv", header = TRUE))
df = unfiltered_df %>%
filter(State == "COMPLETE") %>%
filter("Param.pred_len" != 2) %>%
select(-c(State, `Param.batch_size`, Number,
`UserAttribute.dataset`,
`UserAttribute.granularity`,
`UserAttribute.target`,
`Param.pred_len`))
model <- lm(Value ~ ., data = filtered_df)
summary(model)
x_bar <- mean(df$Value)
n <- length(df$Value)
t_stat <- (x_bar - .5) / (sd(df$Value) / sqrt(length(df$Value)))
p_value <- pt(t_stat, n - 1, lower.tail = FALSE)
# Print t-statistic
# This indicates whether the parameter is significantly different from 0.5
print(t_stat)
df = unfiltered_df %>%
filter(State == "COMPLETE") %>%
filter("Param.pred_len" == 2) %>%
select(-c(State, `Param.batch_size`, Number,
`UserAttribute.dataset`,
`UserAttribute.granularity`,
`UserAttribute.target`,
`Param.pred_len`))
model <- lm(Value ~ ., data = filtered_df)
summary(model)
x_bar <- mean(df$Value)
n <- length(df$Value)
t_stat <- (x_bar - .5) / (sd(df$Value) / sqrt(length(df$Value)))
p_value <- pt(t_stat, n - 1, lower.tail = FALSE)
# Print t-statistic
# This indicates whether the parameter is significantly different from 0.5
print(t_stat)
